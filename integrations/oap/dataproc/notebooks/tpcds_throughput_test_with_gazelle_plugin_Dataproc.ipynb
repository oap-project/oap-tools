{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dcb11a8-11af-4016-85ce-90bd2ce03f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo  \"spark.executorEnv.LD_LIBRARY_PATH=/opt/benchmark-tools/oap/lib\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.executor.extraLibraryPath=/opt/benchmark-tools/oap/lib\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.driver.extraLibraryPath=/opt/benchmark-tools/oap/lib\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.executorEnv.LIBARROW_DIR=/opt/benchmark-tools/oap\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.executorEnv.CC=/opt/benchmark-tools/oap/bin/gcc\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.sql.extensions=com.intel.oap.ColumnarPlugin\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.files=/opt/benchmark-tools/oap/oap_jars/spark-columnar-core-1.2.0-jar-with-dependencies.jar,/opt/benchmark-tools/oap/oap_jars/spark-arrow-datasource-standard-1.2.0-jar-with-dependencies.jar,/opt/benchmark-tools/spark-sql-perf/target/scala-2.12/spark-sql-perf_2.12-0.5.1-SNAPSHOT.jar\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.driver.extraClassPath=/opt/benchmark-tools/oap/oap_jars/spark-columnar-core-1.2.0-jar-with-dependencies.jar:/opt/benchmark-tools/oap/oap_jars/spark-arrow-datasource-standard-1.2.0-jar-with-dependencies.jar:/opt/benchmark-tools/spark-sql-perf/target/scala-2.12/spark-sql-perf_2.12-0.5.1-SNAPSHOT.jar\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.executor.extraClassPath=/opt/benchmark-tools/oap/oap_jars/spark-columnar-core-1.2.0-jar-with-dependencies.jar:/opt/benchmark-tools/oap/oap_jars/spark-arrow-datasource-standard-1.2.0-jar-with-dependencies.jar:/opt/benchmark-tools/spark-sql-perf/target/scala-2.12/spark-sql-perf_2.12-0.5.1-SNAPSHOT.jar\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.oap.sql.columnar.preferColumnar=true\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.sql.join.preferSortMergeJoin=false\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.oap.sql.columnar.joinOptimizationLevel=12\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.sql.broadcastTimeout=3600\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.executor.memoryOverhead=2989\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.dynamicAllocation.executorIdleTimeout=3600s\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.sql.autoBroadcastJoinThreshold=31457280\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.shuffle.manager=org.apache.spark.shuffle.sort.ColumnarShuffleManager\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.oap.sql.columnar.sortmergejoin=true\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.oap.sql.columnar.preferColumnar=true\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.oap.sql.columnar.joinOptimizationLevel=12\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.sql.autoBroadcastJoinThreshold=31457280\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.kryoserializer.buffer.max=256m\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.executor.memory=4g\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.executor.cores=2\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.sql.adaptive.enabled=true\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.driver.memory=2g\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.network.timeout=3600s\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.oap.sql.columnar.sortmergejoin=true\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.memory.offHeap.enabled=false\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.eventLog.enabled=true\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.executor.instances=4\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.sql.inMemoryColumnarStorage.batchSize=20480\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.driver.maxResultSize=3g\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.sql.sources.useV1SourceList=avro\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.sql.extensions=com.intel.oap.ColumnarPlugin\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.history.fs.cleaner.enabled=true\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.serializer=org.apache.spark.serializer.KryoSerializer\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.sql.columnar.window=true\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.sql.columnar.sort=true\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.dynamicAllocation.executorIdleTimeout=3600s\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.sql.execution.arrow.maxRecordsPerBatch=20480\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.kryoserializer.buffer=64m\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.sql.shuffle.partitions=72\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.sql.parquet.columnarReaderBatchSize=20480\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.shuffle.manager=org.apache.spark.shuffle.sort.ColumnarShuffleManager\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.serializer=org.apache.spark.serializer.KryoSerializer\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.authenticate=false\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.sql.columnar.codegen.hashAggregate=false\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.memory.offHeap.size=3g\">> /etc/spark/conf/spark-defaults.conf\n",
    "!echo  \"spark.sql.warehouse.dir=hdfs:///user/livy\">> /etc/spark/conf/spark-defaults.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5539579-b1df-44ab-9e5a-b4dca2dcbde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -mkdir /user/livy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d562cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.text.SimpleDateFormat;\n",
    "import java.util.Date\n",
    "import java.util.concurrent.Executors\n",
    "import java.util.concurrent.ExecutorService\n",
    "import com.databricks.spark.sql.perf.tpcds.TPCDS\n",
    "import com.databricks.spark.sql.perf.Benchmark.ExperimentStatus\n",
    "\n",
    "val stream_num = 2              // how many streams you want to start \n",
    "val scaleFactor = \"1\"           // data scale 1GB\n",
    "val iterations = 1              // how many times to run the whole set of queries.\n",
    "val format = \"parquet\"          // support parquet or orc\n",
    "val storage = \"hdfs\"            // choose HDFS\n",
    "val bucket_name = \"/user/livy\"  // scala notebook only has the write permission of \"hdfs:///user/livy\" directory    \n",
    "val partitionTables = true      // create partition tables\n",
    "val query_filter = Seq()        // Seq() == all queries\n",
    "//val query_filter = Seq(\"q1-v2.4\", \"q2-v2.4\") // run subset of queries\n",
    "val randomizeQueries = true     // run queries in a random order. Recommended for parallel runs.\n",
    "\n",
    "val current_time = new SimpleDateFormat(\"yyyy-MM-dd-HH:mm:ss\").format(new Date)\n",
    "var resultLocation = s\"${storage}://${bucket_name}/results/tpcds_${format}/${scaleFactor}/${current_time}\"\n",
    "var databaseName = s\"tpcds_${format}_scale_${scaleFactor}_db\"\n",
    "val use_arrow = true            // when you want to use gazella_plugin to run TPC-DS, you need to set it true.\n",
    "\n",
    "if (use_arrow){\n",
    "    val data_path = s\"${storage}://${bucket_name}/datagen/tpcds_${format}/${scaleFactor}\"\n",
    "    resultLocation = s\"${storage}://${bucket_name}/results/tpcds_arrow/${scaleFactor}/\"\n",
    "    databaseName = s\"tpcds_arrow_scale_${scaleFactor}_db\"\n",
    "    val tables = Seq(\"call_center\", \"catalog_page\", \"catalog_returns\", \"catalog_sales\", \"customer\", \"customer_address\", \"customer_demographics\", \"date_dim\", \"household_demographics\", \"income_band\", \"inventory\", \"item\", \"promotion\", \"reason\", \"ship_mode\", \"store\", \"store_returns\", \"store_sales\", \"time_dim\", \"warehouse\", \"web_page\", \"web_returns\", \"web_sales\", \"web_site\")\n",
    "    if (spark.catalog.databaseExists(s\"$databaseName\")) {\n",
    "        println(s\"$databaseName has exists!\")\n",
    "    }else{\n",
    "        spark.sql(s\"create database if not exists $databaseName\").show\n",
    "        spark.sql(s\"use $databaseName\").show\n",
    "        for (table <- tables) {\n",
    "            if (spark.catalog.tableExists(s\"$table\")){\n",
    "                println(s\"$table has exists!\")\n",
    "            }else{\n",
    "                spark.catalog.createTable(s\"$table\", s\"$data_path/$table\", \"arrow\")\n",
    "            }\n",
    "        }\n",
    "        if (partitionTables) {\n",
    "            for (table <- tables) {\n",
    "                try{\n",
    "                    spark.sql(s\"ALTER TABLE $table RECOVER PARTITIONS\").show\n",
    "                }catch{\n",
    "                        case e: Exception => println(e)\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "val timeout = 60 // timeout in hours\n",
    "val tpcds = new TPCDS (sqlContext = spark.sqlContext)\n",
    "spark.conf.set(\"spark.sql.broadcastTimeout\", \"10000\") // good idea for Q14, Q88.\n",
    "sql(s\"use $databaseName\")\n",
    "\n",
    "def queries = {\n",
    "    val filtered_queries = query_filter match {\n",
    "        case Seq() => tpcds.tpcds2_4Queries\n",
    "        case _ => tpcds.tpcds2_4Queries.filter(q => query_filter.contains(q.name))\n",
    "    }\n",
    "    if (randomizeQueries) scala.util.Random.shuffle(filtered_queries) else filtered_queries\n",
    "}\n",
    "\n",
    "class ThreadStream(experiment:ExperimentStatus) extends Thread{\n",
    "    override def run(){\n",
    "        println(experiment.toString)\n",
    "        experiment.waitForFinish(timeout*60*60)\n",
    "    }\n",
    "}\n",
    "\n",
    "val threadPool:ExecutorService=Executors.newFixedThreadPool(stream_num)\n",
    "val experiments:Array[ExperimentStatus] = new Array[ExperimentStatus](stream_num)\n",
    "\n",
    "try {\n",
    "    for(i <- 0 to (stream_num - 1)){\n",
    "        experiments(i) = tpcds.runExperiment(\n",
    "            queries, \n",
    "            iterations = iterations,\n",
    "            resultLocation = resultLocation,\n",
    "            tags = Map(\"runtype\" -> \"benchmark\", \"database\" -> databaseName, \"scale_factor\" -> scaleFactor)\n",
    "        )\n",
    "        threadPool.execute(new ThreadStream(experiments(i)))\n",
    "    }\n",
    "}finally{\n",
    "    threadPool.shutdown()\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
